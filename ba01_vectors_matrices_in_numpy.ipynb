{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Vectors, matrices, tensors</center>\n",
    "### <center>Alfred Galichon (NYU & ScPo)</center>\n",
    "## <center>'math+econ+code' masterclass on optimal transport and economic applications</center>\n",
    "#### <center>With python code examples</center>\n",
    "© 2023 by Alfred Galichon with contributions by Clément Montes. Past and present support from NSF grant DMS-1716489, ERC grant CoG-866274 are acknowledged, as well as inputs from contributors listed [here](http://www.math-econ-code.org/team).\n",
    "\n",
    "**If you reuse material from this masterclass, please cite as:**<br>\n",
    "Alfred Galichon, 'math+econ+code' masterclass on optimal transport and economic applications, January 2022. https://github.com/math-econ-code/mec_optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors, matrices and tensors in NumPy\n",
    "\n",
    "* Unlike R or Matlab, Python has no built-in matrix algebra interface. Fortunately, the NumPy library provides powerful matrix capabilities, on par with R or Matlab. Here is a quick introduction to vectorization, operations on vectors and matrices, higher-dimensional arrays, Kronecker products and sparse matrices, etc. in NumPy.\n",
    "\n",
    "* This is *not* a tutorial on Python itself. They are plenty good ones available on the web.\n",
    "\n",
    "* First, we load numpy (with its widely used alias):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NumPy, an `array` is built from a list of numbers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[3 2 5]\n"
     ]
    }
   ],
   "source": [
    "u = np.array([1,2,3])\n",
    "print(u)\n",
    "v = np.array([3,2,5])\n",
    "print(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can then add arrays as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 4 8]\n"
     ]
    }
   ],
   "source": [
    "print(np.array([1,2,3])+np.array([3,2,5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference between the + operator when applied to numpy arrays vs. when applied to lists:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 3, 2, 5]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[1,2,3]+[3,2,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the latter case, it returns list concatenation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To input matrices in NumPy, one simply inputs a list of rows, which are themselves represented as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 12],\n",
       "       [21, 22],\n",
       "       [31, 32]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[11,12],[21,22],[31,32]])\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shape` attribute of an array indicated the dimension of that array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's change that attribute and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 12, 21, 22, 31, 32])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape=(6)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By removing the shape attribute, or rather, by setting it to $6$ instead of $(3,2)$, we took a glimpse at how the matrix is represented in the computer's memory: the rows ($[11,12],[21,22]$, and $[31,32]$) are listed one after another. This is the *row-major order*, used by default in Python, but also in C, as opposed to the *column-major order*, used in Fortran, R and Matlab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization and memory order\n",
    "\n",
    "* Matrices in all mathematical softwares are represented in a *vectorized* way as a sequence of numbers in the computers memory. This representation can involve either stacking the lines, or stacking the columns.\n",
    "\n",
    "* Different programming languages can use either of the two stacking conventions:\n",
    "    + Stacking the lines (Row-major order) is used by `C`, and is the default convention for Python (NumPy). A matrix $M$ is represented by varying the last index first, i.e. a $2\\times2$ matrix will be represented as $vec_C\\left(M\\right) = \\left(M_{11}, M_{12}, M_{21}, M_{22}\\right).$ \n",
    "    + Stacking the columns (Column-major order) is used by `Fortran`, `Matlab`, `R`, and most underlying core linear algebra libraries (like BLAS). A 2x2x2 3-dimensional array $A$ will be represented by varying the first index first, then the second, i.e. $vec_C\\left(A\\right) = \\left( A_{111}, A_{112}, A_{121}, A_{122}, A_{211}, A_{212}, A_{221}, A_{222} \\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `flatten()` provides the vectorized representation of a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 12, 21, 22, 31, 32])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, NumPy represents matrices by **varying the last index first**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to reshape the matrix `a`, one modifies its `shape` attribute. The following reshapes the matrix `a` into a row vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11, 12, 21, 22, 31, 32]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape = 1,6\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous output evidences the fact that Python uses the row-major order: rows are stacked one after the other. \n",
    "To reshape the vector into a column vector, do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11],\n",
       "       [12],\n",
       "       [21],\n",
       "       [22],\n",
       "       [31],\n",
       "       [32]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.shape = 6,1\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equivalently, one could have set `A.shape=6,-1`, where Python would replace `-1` by the integer needed for the formula to make sense (in this case, `1`). \n",
    "Another way to reshape is to use the method `reshape,` which returns a duplicate of the object with the requested shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A1=\n",
      " [0 1 2 3 4 5]\n",
      "A2=\n",
      " [[0 1]\n",
      " [2 3]\n",
      " [4 5]]\n"
     ]
    }
   ],
   "source": [
    "A1=np.array(range(6))\n",
    "A2 = A1.reshape(3,2)\n",
    "print(\"A1=\\n\", A1)\n",
    "print(\"A2=\\n\",A2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `NumPy` also supports the column-major order, but you have to specifically ask for it, by passing the optional argument `order='F'`, where 'F' stands for `Fortran`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 3],\n",
       "       [1, 4],\n",
       "       [2, 5]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A3 = np.array(range(6)).reshape(3,2, order='F')\n",
    "A3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "\n",
    "We now introduce multi-dimensional arrays or *tensors*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  1.  2.  3.]\n",
      "  [ 4.  5.  6.  7.]\n",
      "  [ 8.  9. 10. 11.]\n",
      "  [12. 13. 14. 15.]\n",
      "  [16. 17. 18. 19.]]\n",
      "\n",
      " [[20. 21. 22. 23.]\n",
      "  [24. 25. 26. 27.]\n",
      "  [28. 29. 30. 31.]\n",
      "  [32. 33. 34. 35.]\n",
      "  [36. 37. 38. 39.]]\n",
      "\n",
      " [[40. 41. 42. 43.]\n",
      "  [44. 45. 46. 47.]\n",
      "  [48. 49. 50. 51.]\n",
      "  [52. 53. 54. 55.]\n",
      "  [56. 57. 58. 59.]]]\n"
     ]
    }
   ],
   "source": [
    "nbx,nby,nbz=3,5,4\n",
    "T_xyz =np.array([i*1.0 for i in range(nbx*nby*nbz) ])\n",
    "T_x_y_z = T_xyz.reshape((nbx,nby,nbz))\n",
    "print(T_x_y_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we are given a list of slices of the z's dimension, which is consistent with the row-major order representation: under that representation, when elements of a tensor are listed, *the last index varies first*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3.]\n",
      "[4. 5. 6. 7.]\n",
      "[ 8.  9. 10. 11.]\n"
     ]
    }
   ],
   "source": [
    "print(T_x_y_z[0,0,:])\n",
    "print(T_x_y_z[0,1,:])\n",
    "print(T_x_y_z[0,2,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the submatrix `T_x_y_z[0,:,:]` by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  2.,  3.],\n",
       "       [ 4.,  5.,  6.,  7.],\n",
       "       [ 8.,  9., 10., 11.],\n",
       "       [12., 13., 14., 15.],\n",
       "       [16., 17., 18., 19.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "T_x_y_z[0,:,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kronecker product\n",
    "\n",
    "\n",
    "Given two matrices $A$ of size $(m \\times n)$ and $B$ of size $p \\times q$, the Kronecker product $A \\otimes B$ is the matrix of size $mp \\times nq$ matrix, defined in blockwise way as:\n",
    "\n",
    "\\begin{align*}\n",
    "A \\otimes B = \\begin{bmatrix}\n",
    "a_{11}B & a_{12}B & \\cdots & a_{1n}B \\\\\n",
    "a_{21}B & a_{22}B & \\cdots & a_{2n}B \\\\\n",
    "\\vdots  & \\vdots  & \\ddots & \\vdots  \\\\\n",
    "a_{m1}B & a_{m2}B & \\cdots & a_{mn}B \n",
    "\\end{bmatrix}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=\n",
      " [[1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]] \n",
      "B=\n",
      " [[1 2 3]\n",
      " [4 5 6]] \n",
      "kron(A,B)=\n",
      " [[1. 2. 3. 0. 0. 0. 0. 0. 0.]\n",
      " [4. 5. 6. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 2. 3. 0. 0. 0.]\n",
      " [0. 0. 0. 4. 5. 6. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 2. 3.]\n",
      " [0. 0. 0. 0. 0. 0. 4. 5. 6.]]\n"
     ]
    }
   ],
   "source": [
    "A = np.eye(3)\n",
    "B = np.array([[1,2,3],[4,5,6]])\n",
    "AXB = np.kron(A, B)\n",
    "print(\"A=\\n\",A,\"\\nB=\\n\",B,'\\nkron(A,B)=\\n',AXB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very important identity is\n",
    "\\begin{align*}\n",
    "vec_C\\left(AXB\\right) = \\left(  A\\otimes B^\\top\\right)  vec_C\\left(X\\right),\n",
    "\\end{align*}\n",
    "where $vec_C$ is the vectorization under the C (row-major) order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vec(A @ X @ B)       =  [11. 16. 21. 11. 19. 27. 18. 24. 30.]\n",
      "kron(A,B.T) @ vec(X) =  [11. 16. 21. 11. 19. 27. 18. 24. 30.]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[3,2],[7,1],[2,4]])\n",
    "print('vec(A @ X @ B)       = ',(A @ X @ B).flatten())\n",
    "print('kron(A,B.T) @ vec(X) = ', np.kron(A,B.T)@(X.flatten() ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "\n",
    "**Broadcasting** allows one to work with tensors of different shapes by adding an extra dimension. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.1 , 1.2 , 1.03],\n",
       "       [2.1 , 2.2 , 2.03],\n",
       "       [3.1 , 3.2 , 3.03]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_i = np.array([1,2,3])\n",
    "v_j = np.array([0.1,0.2,.03])\n",
    "u_i[:,None]+v_j[None,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subject to certain constraints, the smaller array is “broadcasted” across the larger array so that they have compatible shapes. Broadcasting provides a means of vectorizing array operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A=\n",
      " [[10]\n",
      " [20]\n",
      " [30]]\n",
      "B=\n",
      " [1 2]\n",
      "A+B=\n",
      " [[11 12]\n",
      " [21 22]\n",
      " [31 32]]\n"
     ]
    }
   ],
   "source": [
    "A = 10*np.array([[1],[2],[3]]) #Simplest broadcasting\n",
    "B =  np.array([1,2])\n",
    "print('A=\\n',A)\n",
    "print('B=\\n',B)\n",
    "print('A+B=\\n',A+B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operation `A[:,np.newaxis]` creates a new dimension. In fact, np.newaxis is a constant whose value equals `None`, so `A[:,None]` yiels the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 4 5]\n",
      "[[3]\n",
      " [4]\n",
      " [5]]\n",
      "[[3 4 5]]\n",
      "[[3 4 5]]\n"
     ]
    }
   ],
   "source": [
    "v = np.array([3,4,5])\n",
    "print(v)\n",
    "print(v[:,np.newaxis])\n",
    "print(v[np.newaxis,:])\n",
    "print(v[None,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One can decrease the dimension of an array, by summing all entries across a given diemnsion.\n",
    "This is done by:\n",
    "\n",
    "$\n",
    "(\\sum_{y}T_{xyz})_{xz}=\\left( I_{X}\\otimes 1^\\top_{X}\\otimes I_{Y}\\right) T$<br>\n",
    "and $\\left( I_{X}\\otimes 1^\\top_{Y}\\otimes I_{Z}\\right)$  is a $XZ \\times XYZ$ matrix.\n",
    "\n",
    "\n",
    "This is equivalent with numpy's partial sum `T_x_y_z.sum(axis = 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 40.  45.  50.  55. 140. 145. 150. 155. 240. 245. 250. 255.]\n",
      "[ 40.  45.  50.  55. 140. 145. 150. 155. 240. 245. 250. 255.]\n"
     ]
    }
   ],
   "source": [
    "print(T_x_y_z.sum(axis=1).flatten())\n",
    "print(np.kron(np.eye(nbx), np.kron(np.ones((1,nby) ), np.eye(nbz)) ) @ T_x_y_z.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expansion\n",
    "\n",
    "Now, $1_{Y}$ is the column vector of dimension $|Y|$ with unit entries. \n",
    "The matrix $\\left( I_{X}\\otimes 1_{Y} \\otimes I_{Z}\\right)$  is a $XYZ \\times XZ$ matrix, which creates an extra dimension.\n",
    "\n",
    "\n",
    "We have for T a tensor of dimension (nbx,nbz):<br>\n",
    "$ \\left( I_{X}\\otimes   1_{Y} \\otimes I_{Y}\\right) T = \\bar{T} $<br>\n",
    "where $\\bar{T}_{xyz} = T_{xz}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1., 2., 3.],\n",
       "        [1., 2., 3.]],\n",
       "\n",
       "       [[4., 5., 6.],\n",
       "        [4., 5., 6.]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B = np.array([[1,2,3],[4,5,6]])\n",
    "nbx,nbz = B.shape\n",
    "nby=2\n",
    "(np.kron(np.kron(np.eye(nbx),np.ones((nby,1))),np.eye(nbz)) @ B.flatten() ). reshape( (nbx,nby,nbz) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1. 2. 3.]\n",
      "  [1. 2. 3.]\n",
      "  [1. 2. 3.]\n",
      "  [1. 2. 3.]]\n",
      "\n",
      " [[4. 5. 6.]\n",
      "  [4. 5. 6.]\n",
      "  [4. 5. 6.]\n",
      "  [4. 5. 6.]]]\n",
      "[[[1. 2. 3.]\n",
      "  [1. 2. 3.]\n",
      "  [1. 2. 3.]\n",
      "  [1. 2. 3.]]\n",
      "\n",
      " [[4. 5. 6.]\n",
      "  [4. 5. 6.]\n",
      "  [4. 5. 6.]\n",
      "  [4. 5. 6.]]]\n"
     ]
    }
   ],
   "source": [
    "nbx,nby,nbz = 2,4,3\n",
    "T2_x_z = np.array([[1,2,3],[4,5,6]]) \n",
    "print( (np.kron(np.eye(nbx), np.kron(np.ones((nby,1) ), np.eye(nbz)) ) @ T2_x_z.flatten()).reshape((nbx,nby,nbz)))\n",
    "print(T2_x_z[:,None,:]+np.zeros((nbx,nby,nbz)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplication "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication of tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several ways to multiply two tensors using NumPy. The most commonly used is the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3., 3.],\n",
       "       [3., 3.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.ones((2,2))\n",
    "B = 3*np.eye(2)\n",
    "A@B #@ is left associative. If you have A@B@C, it will compute (A@B)@C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `np.matmul(A,B)` would give the same result as well, but it is more difficult to read `np.matmul(A,np.matmul(B,C))` than `A@B@C`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplication by a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 0.],\n",
       "       [0., 4.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4*np.eye(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above assignation of B corresponds to the multiplication by a scalar. It is the simplest broadcasting allowed by numpy (which makes this library more powerful than just using lists -it is also much quicker-). More on broadcasting will arrive later in that Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparse matrices in Scipy\n",
    "\n",
    "Sparse matrices are available in the `sparse` module of the `scipy` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as spr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of sparse identity matrix of size 1000 in MB = 0.00095367431640625\n",
      "size of dense identity matrix of size 1000 in MB  = 7.62939453125\n"
     ]
    }
   ],
   "source": [
    "n = 1000\n",
    "\n",
    "print('size of sparse identity matrix of size '+str(n) +' in MB = ' + str(spr.identity(n).data.size  / (1024**2)))\n",
    "\n",
    "print('size of dense identity matrix of size '+str(n) +' in MB  = ' + str(spr.identity(n).todense().nbytes  / (1024**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Working with sparse matrices requires less storage. It is explained by the fact that while a dense matrix needs to encode every coefficient on a byte, sparse matrices only store the non-null coefficients. It is really convenient to work with such objects when it comes to matrices with really high sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 8000000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spr.identity(1000).data.size  , spr.identity(1000).todense().nbytes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating sparse matrices..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... with standard forms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements (1 diagonals) in DIAgonal format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I5 = spr.identity(5) # spr.eye(5) also works\n",
    "I5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can convert your sparse matrix into a dense one in order to visualise it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I5.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... from a dense matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a dense matrix and make it sparse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import uniform module to create random numbers\n",
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.74908024, 1.90142861, 1.46398788, 1.19731697],\n",
       "       [0.31203728, 0.31198904, 0.11616722, 1.73235229],\n",
       "       [1.20223002, 1.41614516, 0.04116899, 1.9398197 ],\n",
       "       [1.66488528, 0.42467822, 0.36364993, 0.36680902]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(seed=42)\n",
    "dense_matrix = uniform.rvs(size=16, loc = 0, scale=2) #List of 16 random draws between 0 and 2\n",
    "dense_matrix = np.reshape(dense_matrix, (4, 4))\n",
    "dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.90142861, 1.46398788, 1.19731697],\n",
       "       [0.        , 0.        , 0.        , 1.73235229],\n",
       "       [1.20223002, 1.41614516, 0.        , 1.9398197 ],\n",
       "       [1.66488528, 0.        , 0.        , 0.        ]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_matrix[dense_matrix < 1] = 0 #Arbitrar criterion\n",
    "dense_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1.9014286128198323\n",
      "  (0, 2)\t1.4639878836228102\n",
      "  (0, 3)\t1.1973169683940732\n",
      "  (1, 3)\t1.7323522915498704\n",
      "  (2, 0)\t1.2022300234864176\n",
      "  (2, 1)\t1.416145155592091\n",
      "  (2, 3)\t1.9398197043239886\n",
      "  (3, 0)\t1.6648852816008435\n"
     ]
    }
   ],
   "source": [
    "sparse_matrix = spr.csr_matrix(dense_matrix)\n",
    "print(sparse_matrix) #It prints a tuple giving the row and columns of the non-null component and its value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ... from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create two arrays containing respectively the rows and the column of the non-null coefficients.\n",
    "A third array would give the value of the non-null coefficient. The result is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 2)\t2.0\n",
      "  (1, 4)\t3.0\n",
      "  (3, 3)\t4.0\n",
      "  (4, 4)\t5.0\n"
     ]
    }
   ],
   "source": [
    "# row indices\n",
    "row_ind = np.array([0, 1, 1, 3, 4])\n",
    "# column indices\n",
    "col_ind = np.array([0, 2, 4, 3, 4])\n",
    "# coefficients\n",
    "data = np.array([1, 2, 3, 4, 5], dtype=float)\n",
    "\n",
    "mat_coo = spr.coo_matrix((data, (row_ind, col_ind)))\n",
    "print(mat_coo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every common operation seen below works with sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2., 1., 1., 1., 1.],\n",
       "        [1., 2., 1., 1., 1.],\n",
       "        [1., 1., 2., 1., 1.],\n",
       "        [1., 1., 1., 2., 1.],\n",
       "        [1., 1., 1., 1., 2.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I5 = spr.identity(5)\n",
    "I5 + np.ones((5,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[2., 0., 0., 0., 0.],\n",
       "        [0., 3., 0., 0., 0.],\n",
       "        [0., 0., 4., 0., 0.],\n",
       "        [0., 0., 0., 5., 0.],\n",
       "        [0., 0., 0., 0., 6.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I5 + np.diag([1.,2.,3.,4.,5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0.],\n",
       "       [0., 2., 0., 0., 0.],\n",
       "       [0., 0., 3., 0., 0.],\n",
       "       [0., 0., 0., 4., 0.],\n",
       "       [0., 0., 0., 0., 5.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I5 @ np.diag([1.,2.,3.,4.,5.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "kron_product = spr.kron(I5 , 10 * np.array([[1,2],[3,4]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[10., 20.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [30., 40.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0., 10., 20.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0., 30., 40.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0., 10., 20.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0., 30., 40.,  0.,  0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0., 10., 20.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0., 30., 40.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 10., 20.],\n",
       "        [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0., 30., 40.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kron_product.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4],\n",
       "        [5, 6]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_i_j = torch.tensor( [[1,2],[3,4],[5,6]])\n",
    "A_i_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 4, 6])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_i_j[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 3],\n",
       "        [9, 6],\n",
       "        [9, 9]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "B_i_j = torch.tensor( [[2,1],[6,2],[4,3]])\n",
    "A_i_j + B_i_j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic differentiation\n",
    "\n",
    "Assume you want to compute the value of $y = (x_0+1)^2+2*x_1$ when $(x_0,x_1)=(2,1)$. In order to do that, torch builds a tree which is represented as:\n",
    "\n",
    "y=sum<br>\n",
    "├──&nbsp;**2<br>\n",
    "│&nbsp;&nbsp;&nbsp;└──&nbsp;sum<br>\n",
    "│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;├──&nbsp;1<br>\n",
    "│&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;└──&nbsp;x_0<br>\n",
    "└──&nbsp;mul<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;├──&nbsp;2<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;└──&nbsp;x_1<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the *value* of $y$, we do *forward propagation*: we input the values of $x_0$ and of $x_1$ at the leaves, and we move up the tree towards the root. This is done by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_i = torch.tensor([2.0,1.0])\n",
    "y = (x_i[0]+1)**2 + 2 * x_i[1]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, in order to compute the derivatives of $y$ with respect to the entries of $x$, we start at the root, and we descend the tree towards the leaves. This can be done automatically in `torch` -- you simply need to add one parameter `requires_grad = True` when setting up the tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 2.])\n"
     ]
    }
   ],
   "source": [
    "x_i =  torch.tensor([2.0,1.0],requires_grad = True)\n",
    "y = (x_i[0]+1)**2+2 * x_i[1]\n",
    "y.backward()\n",
    "print(x_i.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the `backward` command computed the gradient, which is stored in a `grad` attribute. Here is a glimpse of how this network is actually organized:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((<PowBackward0 object at 0x0000020F76AB7D30>, 0), (<MulBackward0 object at 0x0000020F76AB7C40>, 0))\n",
      "((<AddBackward0 object at 0x0000020F76AB7FA0>, 0),)\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn.next_functions)\n",
    "print(y.grad_fn.next_functions[0][0].next_functions)\n",
    "# etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build slightly more complicated functions, such as the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6.2689, 2.7311])\n"
     ]
    }
   ],
   "source": [
    "f = x_i.sum() - torch.log( torch.exp(x_i).sum())\n",
    "f.backward()\n",
    "print(x_i.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how we can use models from Torch to run a linear regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/500], Loss: 2.1106984615325928\n",
      "Epoch [50/500], Loss: 0.1719595342874527\n",
      "Epoch [100/500], Loss: 0.1351642906665802\n",
      "Epoch [150/500], Loss: 0.10625144094228745\n",
      "Epoch [200/500], Loss: 0.08352325111627579\n",
      "Epoch [250/500], Loss: 0.06565685570240021\n",
      "Epoch [300/500], Loss: 0.05161232873797417\n",
      "Epoch [350/500], Loss: 0.04057201370596886\n",
      "Epoch [400/500], Loss: 0.03189336135983467\n",
      "Epoch [450/500], Loss: 0.02507101185619831\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X = torch.tensor([[1.0], [2.0], [3.0]])\n",
    "Y = torch.tensor([[2.0], [4.0], [6.0]])\n",
    "\n",
    "# Define a simple regression model\n",
    "model = nn.Linear(1, 1)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(500):\n",
    "    # Forward pass\n",
    "    outputs = model(X)\n",
    "    loss = criterion(outputs, Y)\n",
    "\n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(f'Epoch [{epoch}/500], Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, logistic regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/1000], Loss: 0.8159709572792053\n",
      "Epoch [200/1000], Loss: 0.6501827239990234\n",
      "Epoch [300/1000], Loss: 0.5333539247512817\n",
      "Epoch [400/1000], Loss: 0.4475764334201813\n",
      "Epoch [500/1000], Loss: 0.38273656368255615\n",
      "Epoch [600/1000], Loss: 0.33249256014823914\n",
      "Epoch [700/1000], Loss: 0.29270243644714355\n",
      "Epoch [800/1000], Loss: 0.26058661937713623\n",
      "Epoch [900/1000], Loss: 0.23423263430595398\n",
      "Epoch [1000/1000], Loss: 0.21229243278503418\n",
      "Predicted class for input tensor([[2., 3., 4.]]): 0\n",
      "Parameter containing:\n",
      "tensor([[-6.9156e-01, -6.4423e-01,  9.4417e-01],\n",
      "        [-6.3537e-01,  1.0929e+00, -6.5314e-01],\n",
      "        [ 1.0754e+00, -2.5664e-04, -2.4126e-01]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.4769,  0.3183, -0.4636], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Prepare the dataset\n",
    "# Features (X) and Labels (y)\n",
    "# Assume we have 3 features for each instance and 3 classes\n",
    "X = torch.tensor([[1.0, 2.0, 3.0], \n",
    "                  [1.0, 3.0, 2.0], \n",
    "                  [4.0, 5.0, 6.0], \n",
    "                  [6.0, 5.0, 4.0]], requires_grad=True)  # example features\n",
    "\n",
    "y = torch.tensor([0, 1, 2, 2])  # example labels (3 classes: 0, 1, and 2)\n",
    "\n",
    "# Build the Logistic Regression Model\n",
    "class SoftmaxRegressionModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SoftmaxRegressionModel, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, num_classes)  # Output size is num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)  # No softmax here, PyTorch's cross-entropy function includes it\n",
    "        return out\n",
    "\n",
    "input_size = X.shape[1]  # Number of input features\n",
    "num_classes = 3  # Number of output classes\n",
    "model = SoftmaxRegressionModel(input_size, num_classes)\n",
    "\n",
    "# Define the Loss Function and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # This includes softmax\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training the Model\n",
    "num_epochs = 1000  # Number of iterations\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    outputs = model(X)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = criterion(outputs, y)\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "\n",
    "# Making Predictions\n",
    "with torch.no_grad():  # We don't need gradients for making predictions\n",
    "    new_data = torch.tensor([[2.0, 3.0, 4.0]])  # New data point with 3 features\n",
    "    outputs = model(new_data)\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "    print(f'Predicted class for input {new_data}: {predicted.item()}')\n",
    "\n",
    "for param_tensor in model.parameters():\n",
    "    print(param_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-6.15391882 -3.93751403  7.07746034]\n",
      " [-4.68446332  8.37413249 -5.18121697]\n",
      " [10.83838214 -4.43661846 -1.89624337]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "skl_model = LogisticRegression(multi_class='multinomial', solver='lbfgs',fit_intercept=False,penalty=None)\n",
    "skl_model.fit(X.detach().numpy(), y)\n",
    "print(skl_model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum\n",
      "├── **2\n",
      "│   └── sum\n",
      "│       ├── 1\n",
      "│       └── x_0\n",
      "└── mul\n",
      "    ├── 2\n",
      "    └── x_1\n"
     ]
    }
   ],
   "source": [
    "from anytree import Node, RenderTree\n",
    "\n",
    "root = Node('sum',parent=None)\n",
    "square = Node('**2',parent= root)\n",
    "sum_n = Node('sum',parent=square)\n",
    "one = Node('1',parent= sum_n)\n",
    "x0 = Node('x_0',parent = sum_n)\n",
    "mul = Node('mul',parent = root)\n",
    "two = Node('2', parent = mul)\n",
    "x1 = Node('x_1',parent = mul)\n",
    "\n",
    "for pre, fill, node in RenderTree(root):\n",
    "    print(\"%s%s\" % (pre, node.name))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
